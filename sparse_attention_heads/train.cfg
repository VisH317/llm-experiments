[vocab]
vocab_size = 30522
vocab_path = ../vocab/vocab.txt
n_unused = 993
max_len = 80
dropout_embed = 0.05
# can define custom tokens here: PAD, MASK, CLS, SEP


[transformer]
n_layers = 4
d_model = 512
n_head = 16
n_active = 8
d_attn = 128
d_ff = 2048
dropout = 0.1
route_type = sum
hidden_act = "gelu"

noise = 0.05
noise_step = 0.5

[train]
task = clm # mlm, clm, or clm_rand (takes a random subset from the start to vary values)
batch_size = 16
val_size = 4
n_epochs = 3
lr = 0.0001
val_step = 20
max_ep_len = 100000