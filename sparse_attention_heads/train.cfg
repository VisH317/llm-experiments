[vocab]
vocab_size = 30522
vocab_path = ../vocab/vocab.txt
n_unused = 993
max_len = 64
dropout_embed = 0.05
# can define custom tokens here: PAD, MASK, CLS, SEP


[transformer]
n_layers = 1
d_model = 256
n_head = 8
n_active = 4
d_attn = 128
d_ff = 1024
dropout = 0.1
route_type = sum

[train]
task = clm # mlm, clm, or clm_rand (takes a random subset from the start to vary values)
batch_size = 64
val_size = 4
n_epochs = 3
lr = 0.0003