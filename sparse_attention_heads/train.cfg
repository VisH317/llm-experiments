[vocab]
vocab_size = 30522
vocab_path = "../vocab/vocab.txt"
n_unused = 993
max_len = 64
dropout_embed = 0.05
# can define custom tokens here: PAD, MASK, CLS, SEP


[transformer]
n_layers = 4
d_model = 256
n_head = 16
n_active = 8
d_attn = 128
d_ff = 1048
dropout = 0.1

[train]
task = "clm" # mlm, clm, or clm_rand (takes a random subset from the start to vary values)
batch_size = 64
val_size = 4
n_epochs = 3